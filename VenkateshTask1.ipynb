{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNOXlWYfePg"
      },
      "outputs": [],
      "source": [
        "#INSTALL DEPENDENCIES\n",
        "!pip install -q transformers datasets peft accelerate bitsandbytes evaluate rouge_score bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKING GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "W96GuLhCfhj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING LIBRARIES\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "import json\n",
        "import torch\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "AcbN7MFJfrk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOADING DATASET\n",
        "MEDQUAD = \"lavita/MedQuAD\"\n",
        "MEDQUAD = load_dataset(MEDQUAD)"
      ],
      "metadata": {
        "id": "VPtz3UNYfuGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SPLITTING THE DATASET\n",
        "train = MEDQUAD[\"train\"].train_test_split(test_size=0.3, seed=42)\n",
        "test = train[\"test\"].train_test_split(test_size=0.5, seed=42)"
      ],
      "metadata": {
        "id": "uR8kOn3XgSiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign splits to descriptive variables\n",
        "train_data = train[\"train\"]\n",
        "validation_data = test[\"train\"]\n",
        "test_data = test[\"test\"]"
      ],
      "metadata": {
        "id": "s67HzVMPqo5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print dataset sizes\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Validation size: {len(validation_data)}\")\n",
        "print(f\"Test size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "HKoWECjPqmbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING BASE LINE MODEL\n",
        "BASE_MODEL= \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "med_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "if med_tokenizer.pad_token is None:\n",
        "    med_tokenizer.pad_token = med_tokenizer.eos_token\n",
        "med_base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "med_base_model.config.pad_token_id = med_tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "IW6bTkrVfvTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Model Performance\n",
        "def evaluate_med_model(model, eval_dataset, max_samples=50):\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "    for item in eval_dataset.select(range(min(max_samples, len(eval_dataset)))):\n",
        "        question_text = str(item.get(\"question\", \"\") or \"\").strip()\n",
        "        reference_text = str(item.get(\"answer\", \"\") or \"\").strip()\n",
        "        if not question_text or not reference_text:\n",
        "            continue\n",
        "        predicted_answer = generate_answer(model, question_text)\n",
        "        predicted_answer = str(predicted_answer or \"\").strip()\n",
        "        all_predictions.append(predicted_answer)\n",
        "        all_references.append(reference_text)\n",
        "    if not all_predictions:\n",
        "        print(\"No valid samples found!\")\n",
        "        return {}, {}\n",
        "    rouge_scores = rouge.compute(predictions=all_predictions, references=all_references)\n",
        "    bleu_scores = bleu.compute(\n",
        "        predictions=all_predictions,\n",
        "        references=[[ref] for ref in all_references]\n",
        "    )\n",
        "    return rouge_scores, bleu_scores"
      ],
      "metadata": {
        "id": "8R0e8wE9fvPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading QLoRA (4-bit) TinyLlama Model\n",
        "qlora_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "qlora_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=qlora_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "if med_tokenizer.pad_token is None:\n",
        "    med_tokenizer.pad_token = med_tokenizer.eos_token\n",
        "qlora_model.config.pad_token_id = med_tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "AbzBrcAKfvNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "tiny_lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "med_lora_model = get_peft_model(qlora_model, tiny_lora_config)\n",
        "med_lora_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "wrER0MyAfvIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization & Preprocessing\n",
        "MAX_SEQ_LENGTH = 256\n",
        "def preprocess_med_sample(sample):\n",
        "    question_text = str(sample.get(\"question\", \"\") or \"\").strip()\n",
        "    answer_text = str(sample.get(\"answer\", \"\") or \"\").strip()\n",
        "    if not question_text or not answer_text:\n",
        "        return {\n",
        "            \"input_ids\": [0] * MAX_SEQ_LENGTH,\n",
        "            \"attention_mask\": [0] * MAX_SEQ_LENGTH,\n",
        "            \"labels\": [-100] * MAX_SEQ_LENGTH\n",
        "        }\n",
        "    prompt_text = f\"<|user|>\\n{question_text}\\n<|assistant|>\\n\"\n",
        "    full_text = prompt_text + answer_text\n",
        "    tokenized_output = med_tokenizer(\n",
        "        full_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_SEQ_LENGTH\n",
        "    )\n",
        "    labels = tokenized_output[\"input_ids\"].copy()\n",
        "    prompt_token_ids = med_tokenizer(\n",
        "        prompt_text,\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LENGTH\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "    labels[:len(prompt_token_ids)] = [-100] * len(prompt_token_ids)\n",
        "    tokenized_output[\"labels\"] = labels\n",
        "    return tokenized_output"
      ],
      "metadata": {
        "id": "UbFyB9LxfvEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Train & Validation Datasets\n",
        "train_tokenized_data = train_data.map(\n",
        "    preprocess_med_sample,\n",
        "    remove_columns=train_data.column_names,\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "validation_tokenized_data = validation_data.map(\n",
        "    preprocess_med_sample,\n",
        "    remove_columns=validation_data.column_names,\n",
        "    batched=False\n",
        ")"
      ],
      "metadata": {
        "id": "z5d__YMeBE80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_tokenized_data)"
      ],
      "metadata": {
        "id": "igRgsmXzBHEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=med_tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "med_training_args = TrainingArguments(\n",
        "    output_dir=\"./qlora_medical_model\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "med_trainer = Trainer(\n",
        "    model=med_lora_model,\n",
        "    args=med_training_args,\n",
        "    train_dataset=train_tokenized_data,\n",
        "    eval_dataset=validation_tokenized_data,\n",
        "    data_collator=med_data_collator\n",
        ")\n",
        "med_trainer.train()"
      ],
      "metadata": {
        "id": "j6-ofBkffvCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_lora_model.save_pretrained(\"./qlora_medical_model\")\n",
        "med_tokenizer.save_pretrained(\"./qlora_medical_model\")"
      ],
      "metadata": {
        "id": "rPRBHFK_fu_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Evaluation Metrics\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "bleu_metric = evaluate.load(\"bleu\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def generate_med_answer(model, question_text, max_input_length=256, max_output_tokens=100):\n",
        "    if not question_text:\n",
        "        return \"\"\n",
        "    model.eval()\n",
        "    prompt_text = f\"<|user|>\\n{question_text}\\n<|assistant|>\\n\"\n",
        "    inputs = med_tokenizer(\n",
        "        prompt_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_length\n",
        "    ).to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_output_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=med_tokenizer.pad_token_id\n",
        "        )\n",
        "    decoded_text = med_tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
        "    answer_text = decoded_text.replace(prompt_text, \"\").strip()\n",
        "    return answer_text\n",
        "\n",
        "# Evaluating Model Performance\n",
        "def evaluate_med_model(model, eval_dataset, max_samples=50):\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "    for item in eval_dataset.select(range(min(max_samples, len(eval_dataset)))):\n",
        "        question_text = str(item.get(\"question\", \"\") or \"\").strip()\n",
        "        reference_text = str(item.get(\"answer\", \"\") or \"\").strip()\n",
        "        if not question_text or not reference_text:\n",
        "            continue\n",
        "        predicted_answer = generate_med_answer(model, question_text) or \"\"\n",
        "        all_predictions.append(predicted_answer)\n",
        "        all_references.append(reference_text)\n",
        "    if not all_predictions:\n",
        "        print(\"No valid samples found!\")\n",
        "        return {}, {}\n",
        "    rouge_scores = rouge_metric.compute(predictions=all_predictions, references=all_references)\n",
        "    bleu_scores = bleu_metric.compute(\n",
        "        predictions=all_predictions,\n",
        "        references=[[ref] for ref in all_references]\n",
        "    )\n",
        "    return rouge_scores, bleu_scores"
      ],
      "metadata": {
        "id": "L557SKYX8Jsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QLoRA Evaluation\n",
        "qlora_rouge_scores, qlora_bleu_scores = evaluate_med_model(med_lora_model, test_data)\n",
        "metrics_output = {\n",
        "    \"rouge\": qlora_rouge_scores,\n",
        "    \"bleu\": qlora_bleu_scores\n",
        "}\n",
        "def convert_numpy(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy(v) for v in obj]\n",
        "    elif isinstance(obj, np.generic):\n",
        "        return obj.item()\n",
        "    else:\n",
        "        return obj\n",
        "clean_metrics = convert_numpy(metrics_output)\n",
        "print(\"\\nQLoRA ROUGE Scores:\")\n",
        "pprint(clean_metrics[\"rouge\"], width=60)\n",
        "\n",
        "print(\"\\nQLoRA BLEU Scores:\")\n",
        "pprint(clean_metrics[\"bleu\"], width=60)\n",
        "\n",
        "with open(\"qlora_medical_metrics.json\", \"w\") as metrics_file:\n",
        "    json.dump(clean_metrics, metrics_file, indent=4)"
      ],
      "metadata": {
        "id": "qbnqeNjjf8Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_rouge_scores, baseline_bleu_scores = evaluate_med_model(med_base_model, test_data)\n",
        "# Baseline Evaluation\n",
        "def convert_numpy(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy(v) for v in obj]\n",
        "    elif isinstance(obj, np.generic):\n",
        "        return float(obj)\n",
        "    else:\n",
        "        return obj\n",
        "clean_baseline = {\n",
        "    \"rouge\": convert_numpy(baseline_rouge_scores),\n",
        "    \"bleu\": convert_numpy(baseline_bleu_scores)\n",
        "}\n",
        "print(\"\\nBaseline ROUGE Scores:\")\n",
        "pprint(clean_baseline[\"rouge\"], width=60)\n",
        "\n",
        "print(\"\\nBaseline BLEU Scores:\")\n",
        "pprint(clean_baseline[\"bleu\"], width=60)"
      ],
      "metadata": {
        "id": "yJjwUSEs9OvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Baseline vs QLoRA Model\n",
        "comparison_results = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline (Phi-2)\", \"QLoRA Fine-Tuned\"],\n",
        "    \"ROUGE-L\": [\n",
        "        baseline_rouge_scores.get(\"rougeL\", 0.0),\n",
        "        qlora_rouge_scores.get(\"rougeL\", 0.0)\n",
        "    ],\n",
        "    \"BLEU\": [\n",
        "        baseline_bleu_scores.get(\"bleu\", 0.0),\n",
        "        qlora_bleu_scores.get(\"bleu\", 0.0)\n",
        "    ]\n",
        "})\n",
        "print(comparison_results)\n",
        "comparison_results.to_csv(\"med_model_comparison.csv\", index=False)"
      ],
      "metadata": {
        "id": "XGgwWxNvf9Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison\n",
        "sns.set(style=\"whitegrid\")\n",
        "plot_data = comparison_results.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=plot_data, palette=\"Set2\")\n",
        "plt.title(\"Baseline vs QLoRA Fine-Tuned Model Performance\")\n",
        "plt.ylim(0, 1)\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xlabel(\"Metric\")\n",
        "plt.legend(title=\"Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D_T2vtNoQg9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}